{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D8usSW9Bwv4h"},"source":["# AIR - Exercise in Google Colab\n","\n","## Colab Preparation\n","\n","Open via google drive -> right click: open with Colab\n","\n","**Get a GPU**\n","\n","Toolbar -> Runtime -> Change Runtime Type -> GPU\n","\n","**Mount Google Drive**\n","\n","* Download data and clone your github repo to your Google Drive folder\n","* Use Google Drive as connection between Github and Colab (Could also use direct github access, but re-submitting credentials might be annoying)\n","* Commit to Github locally from the synced drive\n","\n","**Keep Alive**\n","\n","When training google colab tends to kick you out, This might help: https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n","\n","**Get Started**\n","\n","Run the following script to mount google drive and install needed python packages. Pytorch comes pre-installed."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Sfiw_6jZ0uWa"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install allennlp"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"IUVVDw1m2sed"},"outputs":[],"source":["import torch\n","\n","print(\"Version:\",torch.__version__)\n","print(\"Has GPU:\",torch.cuda.is_available()) # check that 1 gpu is available\n","print(\"Random tensor:\",torch.rand(10,device=\"cuda\")) # check that pytorch works "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fvQMmxs0x_x8"},"source":["# Main.py Replacement\n","\n","-> add your code here\n","\n","- Replace *air_test* with your google drive location in the sys.path.append()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Y_IEUP_2-099"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/My Drive/air_test/src')\n","\n","from allennlp.common import Params, Tqdm\n","from allennlp.common.util import prepare_environment\n","prepare_environment(Params({})) # sets the seeds to be fixed\n","\n","import torch\n","\n","from allennlp.data.iterators import BucketIterator\n","from allennlp.data.vocabulary import Vocabulary\n","\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n","\n","from data_loading import *\n","from model_knrm import *\n","from model_conv_knrm import *\n","from model_match_pyramid import *\n","\n","# change paths to your data directory\n","# executing path is /content -> so change paths accordingly\n","config = {\n","    \"vocab_directory\": \"../data/allen_vocab_lower_10\",\n","    \"pre_trained_embedding\": \"../data/glove.42B.300d.txt\",\n","    \"model\": \"knrm\",\n","    \"train_data\":\"../data/triples.train.tsv\",\n","    \"validation_data\":\"../data/tuples.validation.tsv\",\n","    \"test_data\":\"../data/tuples.test.tsv\",\n","}\n","\n","#\n","# data loading\n","#\n","\n","vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n","tokens_embedder = Embedding.from_params(vocab, Params({\"pretrained_file\": config[\"pre_trained_embedding\"],\n","                                                      \"embedding_dim\": 300,\n","                                                      \"trainable\": True,\n","                                                      \"padding_index\":0}))\n","\n","word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})\n","\n","# recommended default params for the models (but you may change them if you want)\n","if config[\"model\"] == \"knrm\":\n","    model = KNRM(word_embedder, n_kernels=11)\n","elif config[\"model\"] == \"conv_knrm\":\n","    model = Conv_KNRM(word_embedder, n_grams=3, n_kernels=11, conv_out_dim=128)\n","elif config[\"model\"] == \"match_pyramid\":\n","    model = MatchPyramid(word_embedder, conv_output_size=[16,16,16,16,16], conv_kernel_size=[[3,3],[3,3],[3,3],[3,3],[3,3]], adaptive_pooling_size=[[36,90],[18,60],[9,30],[6,20],[3,10]])\n","\n","\n","# todo optimizer, loss \n","\n","print('Model',config[\"model\"],'total parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n","print('Network:', model)\n","\n","#\n","# train\n","#\n","\n","_triple_loader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30,tokenizer = WordTokenizer(word_splitter=JustSpacesWordSplitter())) # already spacy tokenized, so that it is faster \n","\n","_iterator = BucketIterator(batch_size=64,\n","                           sorting_keys=[(\"doc_pos_tokens\", \"num_tokens\"), (\"doc_neg_tokens\", \"num_tokens\")])\n","\n","_iterator.index_with(vocab)\n","\n","for epoch in range(2):\n","\n","    for batch in Tqdm.tqdm(_iterator(_triple_loader.read(config[\"train_data\"]), num_epochs=1)):\n","        # todo train loop\n","        pass\n","\n","\n","#\n","# eval (duplicate for validation inside train loop)\n","#\n","\n","_tuple_loader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30) # not spacy tokenized already (default is spacy)\n","_iterator = BucketIterator(batch_size=128,\n","                           sorting_keys=[(\"doc_tokens\", \"num_tokens\"), (\"query_tokens\", \"num_tokens\")])\n","_iterator.index_with(vocab)\n","\n","for batch in Tqdm.tqdm(_iterator(_tuple_loader.read(config[\"test_data\"]), num_epochs=1)):\n","    # todo test loop \n","    # todo evaluation\n","    pass"]}]}